---
title: "Homework 1"
header-includes: \usepackage{multirow}
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
---

```{r, include=FALSE}

library(tidyverse)
library(here)
library(tidyr)
library(knitr)
library(dplyr)
library(ggplot2)
library(dplyr)
library(cowplot)
library(kableExtra)

knitr::opts_chunk$set(tidy = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(echo = FALSE)

```

## Context

This assignment reinforces ideas in Module 1: Reproducible computing in R. We focus specifically on implementing a large scale simulation study, but the assignment will also include components involving bootstrap and parallelization, Git/GitHub, and project organization.


## Due date and submission

Please submit (via Canvas) a PDF knitted from .Rmd. Your PDF should include the web address of the GitHub repo containing your work for this assignment; git commits after the due date will cause the assignment to be considered late.  

R Markdown documents included as part of your solutions must not install packages, and should only load the packages necessary for your submission to knit.



## Points

```{r, echo = FALSE}
tibble(
  Problem = c("Problem 0", "Problem 1.1", "Problem 1.2", "Problem 1.3", "Problem 1.4", "Problem 1.5"),
  Points = c(20, 10, 5, 20, 30, 15)
) %>%
  knitr::kable()
```


## Problem 0 

This “problem” focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

To that end:

* create a public GitHub repo + local R Project; I suggest naming this repo / directory bios731_hw1_YourLastName (e.g. bios731_hw1_wrobel for Julia)
* Submit your whole project folder to GitHub 
* Submit a PDF knitted from Rmd to Canvas. Your solutions to the problem here should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.

## Problem 1

Simulation study: our goal in this homework will be to plan a well-organized simulation study for multiple linear regression and bootstrapped confidence intervals. 


Below is a multiple linear regression model, where we are interested in primarily treatment effect.


$$Y_i = \beta_0 + \beta_{treatment}X_{i1} + \mathbf{Z_i}^T\boldsymbol{\gamma} + \epsilon_i$$


Notation is defined below: 

* $Y_i$: continuous outcome
* $X_{i1}$: treatment group indicator; $X_{i1}=1$ for treated 
* $\mathbf{Z_i}$: vector of potential confounders
* $\beta_{treatment}$: average treatment effect, adjusting for $\mathbf{Z_i}$
* $\boldsymbol{\gamma}$: vector of regression coefficient values for confounders 
* $\epsilon_i$: errors, we will vary how these are defined


In our simulation, we want to 

* Estimate $\beta_{treatment}$ and $se(\hat{\beta}_{treatment})$
  * Evaluate $\beta_{treatment}$ through bias and coverage
  * We will use 3 methods to compute $se(\hat{\beta}_{treatment})$ and coverage:
    1. Wald confidence intervals (the standard approach)
    2. Nonparametric bootstrap percentile intervals
    3. Nonparametric bootstrap $t$ intervals
  * Evaluate computation times for each method to compute a confidence interval

* Evaluate these properties at:
  - Sample size $n \in \{10, 50, 500\}$
  - True values $\beta_{treatment} \in \{0, 0.5, 2\}$
  - True $\epsilon_i$ normally distributed with $\epsilon_i \sim N(0, 2)$
  - True $\epsilon_i$ coming from a right skewed distribution
    - **Hint**: try $\epsilon_i \sim logNormal(0, \log (2))$

* Assume that there are no confounders ($\boldsymbol{\gamma} = 0$)
* Use a full factorial design



### Problem 1.1 ADEMP Structure 

Answer the following questions: 

* How many simulation scenarios will you be running?
We will be running 3 * 3 * 2 = 18 simulation scenarios

* What are the estimand(s)
The estimands are the average treatment effect $\beta_{treatment}$ and the 
standard error of treatment effect $se(\beta_{treatment})$

* What method(s) are being evaluated/compared?
3 methods are being compared - Wald confidence intervals, nonparametric 
bootstrap percentile intervals, and nonparametric boostrap t-intervals.

* What are the performance measure(s)?
The performance measures we are using are bias, coverage, and computation time.


### Problem 1.2 nSim 

Based on desired coverage of 95\% with Monte Carlo error of no more than 1\%, how many simulations ($n_{sim}$) should we perform for each simulation scenario?  Implement this number of simulations throughout your simulation study.

```{r}
num_scenarios <- 18

mc_err <- 0.01
cover <- 0.95
alpha <- 1 - 0.95
n_sim <- (cover * (1 - cover))/mc_err^2
n_sim
```

We should perform 475 simulations for each simulation scenario.

### Problem 1.3 Implementation 


We will execute this full simulation study. For full credit, make sure to implement the following:

* Well structured scripts and subfolders following guidance from `project_organization` lecture
* Use relative file paths to access intermediate scripts and data objects
* Use readable code practices
* Parallelize your simulation scenarios
* Save results from each simulation scenario in an intermediate `.Rda` or `.rds` dataset in a `data` subfolder 
  * Ignore these data files in your `.gitignore` file so when pushing and committing to GitHub they don't get pushed to remote 
* Make sure your folder contains a Readme explaining the workflow of your simulation study
  * should include how files are executed and in what order 
* Ensure reproducibility! I should be able to clone your GitHub repo, open your `.Rproj` file, and run your simulation study

### Problem 1.4 Results summary

Create a plot or table to summarize simulation results across scenarios and methods for each of the following. 

- Bias of $\hat{\beta}$
- Coverage of $\hat{\beta}$
- Distribution of $se(\hat{\beta})$
- Computation time across methods

If creating a plot, I encourage faceting. Include informative captions for each plot and/or table.


```{r}
all_wald_results <- vector("list", num_scenarios)
all_boot_p_results <- vector("list", num_scenarios)
all_boot_t_results <- vector("list", num_scenarios)



# Loop through each scenario file
for (i in 1:num_scenarios) {
  file_path <- here("results", "sim_wald", paste0("scenario_", i, ".RDA"))
  load(file_path)
  
  all_wald_results[[i]] <- all_wald_estim %>% 
    mutate(err_type = ifelse(err_type == 0, "Lognormal", "Normal"))
  
  file_path <- here("results", "sim_boot_percentile", paste0("scenario_", i, ".RDA"))
  load(file_path)
  
  all_boot_p_results[[i]] <- all_boot_percent_estim %>%
      rename(se_beta = se_beta_hat) %>% 
    mutate(err_type = ifelse(err_type == 0, "Lognormal", "Normal"))
  
  file_path <- here("results", "sim_boot_t", paste0("scenario_", i, ".RDA"))
  load(file_path)
  
  all_boot_t_results[[i]] <- all_boot_t_estim %>%
      rename(se_beta = se_beta_hat) %>% 
    mutate(err_type = ifelse(err_type == 0, "Lognormal", "Normal"))

}


```

```{r}
all_biases <- rep(NA, num_scenarios)
var_hat <- rep(NA, num_scenarios)
wald_coverage <- rep(NA, num_scenarios)
boot_p_coverage <- rep(NA, num_scenarios)
boot_t_coverage <- rep(NA, num_scenarios)
wald_time <- rep(NA, num_scenarios)
boot_p_time <- rep(NA, num_scenarios)
boot_t_time <- rep(NA, num_scenarios)
all_n <- rep(NA, num_scenarios)
all_beta_true <- rep(NA, num_scenarios)
all_err_type <- rep(NA, num_scenarios)
scenario_num <- rep(NA, num_scenarios)
mean_wald_se_beta <- rep(NA, num_scenarios)
mean_boot_p_se_beta <- rep(NA, num_scenarios)
mean_boot_t_se_beta <- rep(NA, num_scenarios)
se_hat <- rep(NA, num_scenarios)
se_hat_se <- rep(NA, num_scenarios)
  
for (i in 1:num_scenarios) {
  
  all_biases[i] <- (1/n_sim) * sum(all_wald_results[[i]]$beta_hat - all_wald_results[[i]]$beta_true)
  var_hat[i] <- sd(all_wald_results[[i]]$beta_hat)
  se_hat[i] <- mean(all_wald_results[[i]]$se_beta)
  se_hat_se[i] <- sd(all_wald_results[[i]]$se_beta)
  wald_coverage[i] <- mean(all_wald_results[[i]]$coverage == 1) * 100
  boot_p_coverage[i] <- mean(all_boot_p_results[[i]]$coverage == 1) * 100
  boot_t_coverage[i] <- mean(all_boot_t_results[[i]]$coverage == 1) * 100
  wald_time[i] <- mean(all_wald_results[[i]]$time)
  boot_p_time[i] <- mean(all_boot_p_results[[i]]$time)
  boot_t_time[i] <- mean(all_boot_t_results[[i]]$time)
  all_n[i] <- unique(all_wald_results[[i]]$n)[[1]]
  all_beta_true[i] <- unique(all_wald_results[[i]]$beta_true)
  all_err_type[i] <- unique(all_wald_results[[i]]$err_type)
  mean_wald_se_beta[i] <- mean(all_wald_results[[i]]$se_beta)
  mean_boot_p_se_beta[i] <- mean(all_boot_p_results[[i]]$se_beta)
  mean_boot_t_se_beta[i] <- mean(all_boot_t_results[[i]]$se_beta)

  
  scenario_num[i] <- i
}

df <- bind_cols(
  scenario_num = scenario_num, 
  n = all_n, 
  beta_true = all_beta_true, 
  error_type = all_err_type, 
  bias = all_biases,
  var = var_hat,
  se_hat = se_hat,
  se_hat_se = se_hat_se,
  wald_coverage = wald_coverage,
  wald_time = wald_time,
  wald_se = mean_wald_se_beta,
  boot_p_coverage = boot_p_coverage,
  boot_p_time = boot_p_time,
  boot_p_se = mean_boot_p_se_beta,
  boot_t_coverage = boot_t_coverage,
  boot_t_time = boot_t_time,
  boot_t_se = mean_boot_t_se_beta
  )




```

#### Bias
```{r}
bias_table <- df %>%
  select(n, beta_true, error_type, bias) %>%  
  arrange(n, beta_true) %>%
  rename("N" = n, "True Beta" = beta_true) %>%
  pivot_wider(
    names_from = error_type,
    values_from = bias,   
  )

kable(bias_table, digits = 3, caption = "Bias Summary Table")
```

#### Coverage
```{r}
coverage_table <- df %>%
  select(n, beta_true, error_type, wald_coverage, boot_p_coverage, boot_t_coverage) %>%
  arrange(n, beta_true, error_type) %>%
  pivot_wider(
    names_from = error_type,  
    values_from = c(wald_coverage, boot_p_coverage, boot_t_coverage),
    names_glue = "{error_type} {.value}",  # Dynamically generates column names
  ) %>%
  select(n, beta_true, starts_with("Lognormal"), starts_with("Normal")) %>%
  rename(
    "N" = n,
    "True Beta" = beta_true,
    "Lognormal Wald CI" = "Lognormal wald_coverage",
    "Lognormal Bootstrap Percentile CI" = "Lognormal boot_p_coverage",
    "Lognormal Bootstrap t CI" = "Lognormal boot_t_coverage",
    "Normal Wald CI" = "Normal wald_coverage",
    "Normal Bootstrap Percentile CI" = "Normal boot_p_coverage",
    "Normal Bootstrap t CI" = "Normal boot_t_coverage"
  )


kable(coverage_table, digits = 3, caption = "Coverage Summary Table") %>%
  add_header_above(c(" " = 2, "Lognormal" = 3, "Normal" = 3)) %>%
  column_spec(1, width = "1cm") %>%
  column_spec(2, width = "1cm") %>%
  column_spec(3:5, width = "2cm") %>%
  column_spec(6:8, width = "2cm")

```

#### Computation Time
```{r}
time_table <- df %>%
  select(n, beta_true, error_type, wald_time, boot_p_time, boot_t_time) %>%
  arrange(n, beta_true, error_type) %>%
  pivot_wider(
    names_from = c(error_type),
    values_from = c(wald_time, boot_p_time, boot_t_time),
    names_glue = "{error_type} {.value}", 
  ) %>%
  select(n, beta_true, starts_with("Lognormal"), starts_with("Normal")) %>%
  rename(
    "N" = n,
    "True Beta" = beta_true,
    "Lognormal Wald Time" = "Lognormal wald_time",
    "Lognormal Bootstrap Percentile Time" = "Lognormal boot_p_time",
    "Lognormal Bootstrap t Time" = "Lognormal boot_t_time",
    "Normal Wald Time" = "Normal wald_time",
    "Normal Bootstrap Percentile Time" = "Normal boot_p_time",
    "Normal Bootstrap t Time" = "Normal boot_t_time"
  )

kable(time_table, digits = 3, caption = "Computation Time Summary Table") %>%
  add_header_above(c(" " = 2, "Lognormal" = 3, "Normal" = 3)) %>%
  column_spec(1, width = "1cm") %>%
  column_spec(2, width = "1cm") %>%
  column_spec(3:5, width = "2cm") %>%
  column_spec(6:8, width = "2cm")
```



#### Standard Error of Beta (Distribution)
```{r}
all_results <- vector("list", num_scenarios)

for (i in 1:num_scenarios) {
  all_results[[i]] <- bind_rows(
    all_wald_results[[i]] %>%
      mutate(method = "Wald"),
    all_boot_p_results[[i]] %>%
      mutate(method = "Bootstrap Percentile"),
    all_boot_t_results[[i]] %>%
      mutate(method = "Bootstrap t") 
  )
}

combined_df <- bind_rows(all_results)

wald_plot <- ggplot(filter(combined_df, method == "Wald"), aes(x = se_beta)) +
  geom_histogram(fill = "blue", alpha = 0.5, bins = 30) +
  facet_grid(n ~ err_type + beta_true, scales = "free_y") +  
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 8, angle = 45, hjust = 1)
  ) +
  labs(title = "Wald Method", x = "se_beta", y = "Count") +
  coord_cartesian(xlim = c(0, 2)) 

boot_p_plot <- ggplot(filter(combined_df, method == "Bootstrap Percentile"), aes(x = se_beta)) +
  geom_histogram(fill = "green", alpha = 0.5, bins = 30) +
  facet_grid(n ~ err_type + beta_true, scales = "free_y") + 
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 8, angle = 45, hjust = 1)
  ) +
  labs(title = "Bootstrap Percentile Method", x = "se_beta", y = "Count") +
  coord_cartesian(xlim = c(0, 2)) 

boot_t_plot <- ggplot(filter(combined_df, method == "Bootstrap t"), aes(x = se_beta)) +
  geom_histogram(fill = "red", alpha = 0.5, bins = 30) +
  facet_grid(n ~ err_type + beta_true, scales = "free_y") + 
  theme_minimal() +
  theme(
    axis.text.x = element_text(size = 8, angle = 45, hjust = 1)
  ) +
  labs(title = "Bootstrap t Method", x = "se_beta", y = "Count") +
  coord_cartesian(xlim = c(0, 2))  


print(wald_plot)
print(boot_p_plot)
print(boot_t_plot)


```



### Problem 1.5 Discussion

Interpret the results summarized in Problem 1.4. First, write a **paragraph** summarizing the main findings of your simulation study. Then, answer the specific questions below.

- How do the different methods for constructing confidence intervals compare in terms of computation time?
- Which method(s) for constructing confidence intervals provide the best coverage when $\epsilon_i \sim N(0, 2)$?
- Which method(s) for constructing confidence intervals provide the best coverage when $\epsilon_i \sim logNormal(0, \log (2))$?
